{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7gehl58lO1X"
      },
      "source": [
        "# **Fine-Tuning AI Models with LoRA and Deploying with Streamlit**\n",
        "## **Hands-On Workshop**\n",
        "### **Duration: 45 minutes**\n",
        "\n",
        "This hands-on session covers fine-tuning AI models using **LoRA (Low-Rank Adaptation)** and deploying them using **Streamlit**.\n",
        "\n",
        "### **Objectives:**\n",
        "- Understand LoRA and its impact on efficient model fine-tuning.\n",
        "- Apply LoRA fine-tuning to AI models based on project requirements.\n",
        "- Fine-tune models including **GPT-2, BERT, Whisper, and Stable Diffusion**.\n",
        "- Build and deploy an interactive **Streamlit web application**.\n",
        "- Customize LoRA models for real-world project applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqzh5x-llO1Y"
      },
      "source": [
        "## **Step 1: Install Dependencies**\n",
        "First, install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-8qzSSMUlO1Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (4.48.1)\n",
            "Requirement already satisfied: peft in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (0.14.0)\n",
            "Requirement already satisfied: accelerate in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (1.3.0)\n",
            "Requirement already satisfied: streamlit in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (1.42.0)\n",
            "Requirement already satisfied: diffusers in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (0.32.2)\n",
            "Requirement already satisfied: torch in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (2.6.0)\n",
            "Requirement already satisfied: torchaudio in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (2.6.0)\n",
            "Requirement already satisfied: filelock in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (0.28.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from peft) (5.8.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (5.5.1)\n",
            "Requirement already satisfied: click<9,>=7.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (2.2.3)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (11.1.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (5.29.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (19.0.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: importlib-metadata in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from diffusers) (8.6.1)\n",
            "Requirement already satisfied: networkx in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit) (1.24.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers peft accelerate streamlit diffusers torch torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ST6xB4lO1a"
      },
      "source": [
        "## **Step 2: Select and Load Your Model**\n",
        "Choose the model based on your project:\n",
        "- **GPT-2** for text generation.\n",
        "- **BERT** for text classification.\n",
        "- **Whisper** for speech-to-text.\n",
        "- **Stable Diffusion** for text-to-image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fIT_1FC-lO1a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModelForSpeechSeq2Seq\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Choose model\n",
        "model_choice = 'gpt2'  # Change to 'bert', 'whisper', or 'stable-diffusion' as needed\n",
        "\n",
        "if model_choice == 'gpt2':\n",
        "    model_name = 'gpt2'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "elif model_choice == 'bert':\n",
        "    model_name = 'bert-base-uncased'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "elif model_choice == 'whisper':\n",
        "    model_name = 'openai/whisper-small'\n",
        "    tokenizer = None\n",
        "    model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name)\n",
        "elif model_choice == 'stable-diffusion':\n",
        "    model_name = 'runwayml/stable-diffusion-v1-5'\n",
        "    tokenizer = None\n",
        "    model = StableDiffusionPipeline.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u61lz1LqlO1a"
      },
      "source": [
        "## **Step 3: Apply LoRA Fine-Tuning**\n",
        "Fine-tune the model using LoRA to improve efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ha8jyZVImWa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Apply LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"c_attn\"],  # Changed target modules to 'c_attn'\n",
        "    task_type=\"CAUSAL_LM\"  # Add task type for causal language modeling\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHccznoJlO1a"
      },
      "source": [
        "## **Step 4: Test Fine-Tuned Model**\n",
        "Provide sample inputs to test the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mCcLWzSmlO1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The future of AI is uncertain. The future of AI is uncertain.\n",
            "\n",
            "The future of AI is uncertain. The future of AI is uncertain.\n",
            "\n",
            "The future of AI is uncertain. The future of AI is uncertain.\n",
            "\n",
            "The future\n"
          ]
        }
      ],
      "source": [
        "# Example for GPT-2\n",
        "if model_choice == 'gpt2':\n",
        "    prompt = \"The future of AI is\"\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    output = model.generate(input_ids, max_length=50)\n",
        "    print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz1iZ7RDlO1a"
      },
      "source": [
        "## **Step 5: Deploy as a Streamlit Web App**\n",
        "Now, create a simple **Streamlit web interface** for model interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KnN5ge2olO1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "st.title('LoRA Fine-Tuned Model Web Interface')\n",
        "\n",
        "# Load model\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "\n",
        "# User input\n",
        "prompt = st.text_input('Enter your prompt:')\n",
        "\n",
        "if st.button('Generate Text'):\n",
        "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_length=50, do_sample=True)\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    st.write(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOqN26JQlO1b"
      },
      "source": [
        "## **Step 6: Run the Streamlit App**\n",
        "Run the following command in Colab to launch the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QGtfCB28lO1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.0.80.179:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
            "\n",
            "  $ xcode-select --install\n",
            "  $ pip install watchdog\n",
            "            \u001b[0m\n",
            "^C\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in: <module 'threading' from '/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1447, in _shutdown\n",
            "    atexit_call()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/concurrent/futures/thread.py\", line 31, in _python_exit\n",
            "    t.join()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/web/bootstrap.py\", line 44, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/web/server/server.py\", line 470, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/runtime/runtime.py\", line 337, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/asyncio/base_events.py\", line 796, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/asyncio/base_events.py\", line 515, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRL5ULTolO1b"
      },
      "source": [
        "## **Step 7: Customize for Your Project**\n",
        "Participants should adapt LoRA fine-tuning and Streamlit deployment based on their specific project requirements.\n",
        "\n",
        "### **Customizing LoRA for Your Project:**\n",
        "- Adjust LoRA parameters such as rank and dropout based on dataset size.\n",
        "- Train with domain-specific data to improve model accuracy.\n",
        "\n",
        "### **Enhancing the Web Interface:**\n",
        "- Modify the UI to include more features such as dropdowns and sliders.\n",
        "- Optimize performance by reducing latency and improving text responses.\n",
        "\n",
        "### **Deploying Your Model:**\n",
        "- Consider deploying the model on **Hugging Face Spaces** or **AWS Lambda** for wider accessibility.\n",
        "- Document project results and improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load training dataset\n",
        "train_file_path = \"archive/twitter_training.csv\"\n",
        "df_train = pd.read_csv(train_file_path)\n",
        "\n",
        "# Rename columns based on dataset structure\n",
        "df_train.columns = [\"ID\", \"Category\", \"Sentiment\", \"Text\"]\n",
        "\n",
        "# Drop rows with missing text\n",
        "df_train = df_train.dropna(subset=[\"Text\"])\n",
        "\n",
        "# Map sentiment labels to numerical values\n",
        "label_mapping = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
        "df_train[\"Sentiment\"] = df_train[\"Sentiment\"].map(label_mapping)\n",
        "\n",
        "# Drop rows where sentiment mapping failed (if any)\n",
        "df_train = df_train.dropna(subset=[\"Sentiment\"]).reset_index(drop=True)\n",
        "\n",
        "# Convert Sentiment column to integer type\n",
        "df_train[\"Sentiment\"] = df_train[\"Sentiment\"].astype(int)\n",
        "\n",
        "# Convert DataFrame to Hugging Face Dataset\n",
        "train_dataset = Dataset.from_pandas(df_train[[\"Text\", \"Sentiment\"]])\n",
        "\n",
        "# Load validation dataset\n",
        "val_file_path = \"archive/twitter_validation.csv\"\n",
        "df_val = pd.read_csv(val_file_path)\n",
        "\n",
        "# Rename columns for validation dataset\n",
        "df_val.columns = [\"ID\", \"Category\", \"Sentiment\", \"Text\"]\n",
        "\n",
        "# Drop missing text in validation dataset\n",
        "df_val = df_val.dropna(subset=[\"Text\"])\n",
        "\n",
        "# Apply sentiment mapping\n",
        "df_val[\"Sentiment\"] = df_val[\"Sentiment\"].map(label_mapping)\n",
        "\n",
        "# Drop invalid rows\n",
        "df_val = df_val.dropna(subset=[\"Sentiment\"]).reset_index(drop=True)\n",
        "\n",
        "# Convert Sentiment column to integer type\n",
        "df_val[\"Sentiment\"] = df_val[\"Sentiment\"].astype(int)\n",
        "\n",
        "# Convert Validation DataFrame to Hugging Face Dataset\n",
        "val_dataset = Dataset.from_pandas(df_val[[\"Text\", \"Sentiment\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 61120/61120 [00:16<00:00, 3700.35 examples/s]\n",
            "Map: 100%|██████████| 828/828 [00:00<00:00, 3105.60 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenization function\n",
        "def preprocess_function(examples):\n",
        "    result = tokenizer(examples['Text'], truncation=True, padding='max_length', max_length=128)\n",
        "    result[\"labels\"] = examples[\"Sentiment\"]\n",
        "    return result\n",
        "\n",
        "# Apply tokenization to training and validation datasets\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LoRA Fine-Tuning (BERT Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 294,912 || all params: 109,779,459 || trainable%: 0.2686\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11460' max='11460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11460/11460 30:10, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.931800</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.799300</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.797300</td>\n",
              "      <td>No log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model\n",
        "model_name = \"bert-base-uncased\"\n",
        "base_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 sentiment classes\n",
        "\n",
        "# Define LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # LoRA rank\n",
        "    lora_alpha=16,  # LoRA scaling factor\n",
        "    lora_dropout=0.1,  # Dropout rate for LoRA layers\n",
        "    target_modules=[\"query\", \"value\"],  # Apply LoRA to key transformer layers\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# Print model summary (optional)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    report_to=\"none\",  # Avoid logging to wandb/huggingface unless needed\n",
        ")\n",
        "\n",
        "# Trainer with validation dataset\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,  \n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model\n",
        "trainer.save_model(\"./trained_lora_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streamlit Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.0.80.179:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
            "\n",
            "  $ xcode-select --install\n",
            "  $ pip install watchdog\n",
            "            \u001b[0m\n",
            "Using device: mps\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2025-02-13 15:08:43.448 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "Using device: mps\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "^C\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in: <module 'threading' from '/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py'>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1447, in _shutdown\n",
            "    atexit_call()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/concurrent/futures/thread.py\", line 31, in _python_exit\n",
            "    t.join()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1060, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/threading.py\", line 1080, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/web/bootstrap.py\", line 44, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/web/server/server.py\", line 470, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/site-packages/streamlit/runtime/runtime.py\", line 337, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/asyncio/base_events.py\", line 796, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/Users/nanxuan/miniconda3/envs/dscapstone/lib/python3.9/asyncio/base_events.py\", line 515, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dscapstone",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
